{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import multiprocessing as mp\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "from openie import StanfordOpenIE\n",
    "from itertools import groupby\n",
    "from fuzzywuzzy import fuzz\n",
    "from gender_predictor.GenderClassifier import classify_gender\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm, notebook\n",
    "from IPython.utils import io\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_data_df = pd.read_csv('movie.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'movie_name', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres'])\n",
    "# character_df  = pd.read_csv('character.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'release_date','char_name', 'dob', 'gender', 'height', 'ethnicity', 'name', 'age', 'free_char_id1', 'free_char_id2', 'free_char_id3'])\n",
    "# plot_df       = pd.read_csv('../MovieSummaries/plot_summaries.txt', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'plot'])\n",
    "\n",
    "# character_df  = character_df[['id', 'char_name', 'gender']]\n",
    "\n",
    "# movie_data_df['release_year'] = movie_data_df['release_date'].apply(lambda r:r[:4] if str(r)!='nan' else None)\n",
    "\n",
    "# movie_id_by_year = {'United States of America':{}, 'India':{}, 'United Kingdom':{}}\n",
    "\n",
    "# for index, row in movie_data_df.iterrows():\n",
    "#     for key, value in json.loads(row['countries']).items():            \n",
    "#         if value == 'United States of America' or value == 'India' or value == 'United Kingdom':\n",
    "#             if row['release_year'] not in movie_id_by_year[value]:\n",
    "#                 movie_id_by_year[value][row['release_year']] = [row.id]\n",
    "#             else:\n",
    "#                 movie_id_by_year[value][row['release_year']].append(row.id)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = {\n",
    "    'openie.affinity_probability_cap': 1 / 3,\n",
    "}\n",
    "\n",
    "client = StanfordOpenIE(properties=properties)\n",
    "nlp    = spacy.load('en_core_web_sm')\n",
    "coref  = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    tokens_without_sw = []\n",
    "    for (word, tag) in nltk.pos_tag(nltk.word_tokenize(text)):\n",
    "        if tag == 'NNP' or word[0].isupper():\n",
    "            continue\n",
    "        elif word not in stopwords:\n",
    "            tokens_without_sw.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20153bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreference_movie_plot(nlp, plot):    \n",
    "    doc = nlp(plot)\n",
    "    return doc._.coref_resolved\n",
    "\n",
    "def key_func(k):\n",
    "    return k['subject']\n",
    "\n",
    "def get_triplets(plot, client):\n",
    "    sent_plot = plot.split(\".\")\n",
    "    combined_list = []\n",
    "    sent_dict = {}\n",
    "    for i in sent_plot:\n",
    "        #print(i)\n",
    "        sent = []\n",
    "        for triple in client.annotate(i):\n",
    "            sent.append(triple)\n",
    "\n",
    "        INFO = sorted(sent, key=key_func)\n",
    "\n",
    "\n",
    "        comb_sent = []\n",
    "        for key, value in groupby(INFO, key_func):\n",
    "            if key in sent_dict:\n",
    "                vals = sent_dict[key]\n",
    "                vals.append(list(value)[0])\n",
    "                sent_dict[key] = vals\n",
    "            else:\n",
    "                vals = []\n",
    "                vals.append(list(value)[0])\n",
    "                sent_dict[key] = vals\n",
    "    return sent_dict\n",
    "    \n",
    "def get_event_chain(triplets):\n",
    "    #TODO: Fix if neighboring events are same, this will repeat, remove them\n",
    "    characters = defaultdict(set)\n",
    "    for key, value in triplets.items():\n",
    "        # Check if should use only relation or use object also\n",
    "        events = [val['relation'] + \" \" + val['object'] for val in value]\n",
    "        flag   = False\n",
    "        for char in characters.keys():\n",
    "            if fuzz.ratio(char, key) >= 50:\n",
    "                characters[char].update(events)\n",
    "                flag = True\n",
    "        if not flag:\n",
    "            characters[key].update(events)\n",
    "            \n",
    "    return characters\n",
    "\n",
    "            \n",
    "def get_movies_in_decade(country, zone=\"before\"):\n",
    "    if zone == 'before':\n",
    "        count = [i for i in movie_id_by_year[country].keys() if i is not None and i<'2000']\n",
    "    else:\n",
    "        count = [i for i in movie_id_by_year[country].keys() if i is not None and i>='2012']\n",
    "\n",
    "    decade_2000 = []\n",
    "    for c in count:\n",
    "        decade_2000 += movie_id_by_year[country][c]\n",
    "    return decade_2000\n",
    "\n",
    "def get_bigrams(event_chain):\n",
    "    tokens = nltk.word_tokenize(event_chain)\n",
    "    return list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_words = ['she', 'her', 'woman', 'women', 'ladies', 'girls', 'lady', 'aunt', 'grandmother', 'female', 'girl', 'damsel', 'maiden', 'daughter', 'sister', 'mother']\n",
    "male_words   = ['he', 'his', 'him', 'son', 'man', 'male', 'men', 'boys', 'gentleman', 'uncle', 'grandfather', 'gentlemen', 'boy', 'bloke', 'brother', 'father']\n",
    "gender_neutral = ['they', 'them', 'it', 'theirs', 'i', 'you', 'we']\n",
    "\n",
    "def get_frequency_for_movie(movie):\n",
    "    \n",
    "    #Get triplets using OpenIE on coreferenced movie plot\n",
    "    coref_plot = coreference_movie_plot(nlp, movie)\n",
    "    \n",
    "    triplets = get_triplets(coref_plot, client)\n",
    "    \n",
    "    # Get event chain for each character\n",
    "    event_chains = get_event_chain(triplets)\n",
    "    frequency_list = {'M':list(), 'F':list()}\n",
    "    gender_list    = {}\n",
    "    for character, event_chain in event_chains.items():            \n",
    "        try:\n",
    "            character_name = \"\"\n",
    "            \n",
    "            # TRY 1 to get person - NER\n",
    "            ner = nlp(character).ents\n",
    "            if len(ner)>0:\n",
    "                for entity in ner:\n",
    "                    if entity.label_ == 'PERSON':\n",
    "                        character_name = str(entity)\n",
    "\n",
    "            # TRY 2 - POS TAGGING - Pronouns\n",
    "            if character_name == \"\":\n",
    "                # Verry buggy\n",
    "                # Check difference between using other tools like spacy/pycorenlp\n",
    "                tags = nltk.pos_tag(nltk.word_tokenize(character))\n",
    "\n",
    "                # Find whether the sentence is associated with male or female\n",
    "                # Check if we should directly choose one gender\n",
    "                for (word, tag) in tags:\n",
    "                    if 'NNP' in tag:\n",
    "                        character_name = word.lower()\n",
    "                    if 'PRP' in tag:\n",
    "                        character_name = word.lower()\n",
    "                    if word.lower() in female_words or word.lower() in male_words:\n",
    "                        character_name = word.lower()\n",
    "                    if word[0].isupper():\n",
    "                        character_name = word\n",
    "\n",
    "            \n",
    "            # Don't process if character is not noun or pronoun\n",
    "            if character_name == \"\":\n",
    "                continue\n",
    "            gender = None\n",
    "            if character_name in gender_list:\n",
    "                gender = gender_list[character_name]\n",
    "            elif character_name in female_words:\n",
    "                gender = 'F'\n",
    "            elif character_name in male_words:\n",
    "                gender = 'M'\n",
    "            elif character in gender_neutral:\n",
    "                    continue\n",
    "            if gender is None:\n",
    "                with io.capture_output() as captured:\n",
    "                    gender = classify_gender(character_name)\n",
    "             \n",
    "            gender_list[character_name] = gender\n",
    "            \n",
    "            # Get unigrams\n",
    "            unigrams = lemmatize_words(\" \".join(event_chain))\n",
    "            if len(unigrams)>0:\n",
    "                frequency_list[gender].append(unigrams)\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "            pass\n",
    "    \n",
    "    return frequency_list\n",
    "    \n",
    "def get_frequency_mapping(all_movie_plots, decade):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    pool = mp.Pool(5)\n",
    "#     plots = []\n",
    "#     for key in all_movie_plots.keys():\n",
    "#         if key and key.isdigit() and int(key) >= decade and int(key) < decade+70:\n",
    "#             plots.extend(all_movie_plots[key])\n",
    "    output = {}\n",
    "    for year in notebook.tqdm(all_movie_plots):\n",
    "        results = [pool.apply_async(get_frequency_for_movie, args=(movie,)) for movie in all_movie_plots[year]]    \n",
    "        movie_chain = [p.get() for p in notebook.tqdm(results, leave=False)]\n",
    "        output[year] = movie_chain\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def get_adjective_cloud(plots, decade=None):\n",
    "    frequency_list = get_frequency_mapping(plots, decade)\n",
    "    return frequency_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"UK_plot_summaries_by_year.json\") as f:\n",
    "    movie_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44874815",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_frequency_for_movie(movie_dict['2001'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7249f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_adjective_cloud(movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bb045",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"UK_event_chain_by_each_year.json\", 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
