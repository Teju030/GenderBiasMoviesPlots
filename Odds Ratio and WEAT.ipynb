{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get adjectives related to male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core import indexing\n",
    "from gender_predictor.GenderClassifier import classify_gender\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "character = pd.read_csv('character.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'release_date','char_name', 'dob', 'gender', 'height', 'ethnicity', 'name', 'age', 'free_char_id1', 'free_char_id2', 'free_char_id3'])\n",
    "character = character[['id', 'char_name', 'gender']]\n",
    "\n",
    "\n",
    "female_words = ['she', 'her', 'woman', 'lady', 'female', 'girl', 'damsel', 'maiden', 'daughter', 'sister', 'mother']\n",
    "male_words = ['he', 'his', 'man', 'male', 'gentleman', 'boy', 'bloke', 'brother', 'father']\n",
    "\n",
    "def read_input_file(filename):\n",
    "    data_df = pd.read_csv(filename,sep=',', skip_blank_lines=True, index_col= False)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_plots_by_movie_id(data_df):\n",
    "    movie_ids = data_df.movie_id.unique() \n",
    "    grouped = data_df.groupby(data_df.movie_id)\n",
    "\n",
    "    all_movie_plots = []\n",
    "    for id in movie_ids:\n",
    "        sents_df = grouped.get_group(id)\n",
    "        all_movie_plots.append(sents_df)\n",
    "    return all_movie_plots[:1000]\n",
    "\n",
    "\n",
    "def get_frequency_for_movie(movie):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    \n",
    "    # Add woman related words like woman, man, lady\n",
    "    name_data  = movie[(movie.dep_pos == 'PRP') |  ((movie.dep_pos == 'NNP') & (movie.dep_ner == 'PERSON')) | (movie.dependent.isin(female_words)) | (movie.dependent.isin(male_words))]\n",
    "    char_list  = character[character.id==movie.iloc[0]['movie_id']]\n",
    "\n",
    "    for idx,name in name_data.iterrows():            \n",
    "        try:\n",
    "            character_name = name['dependent'].lower()\n",
    "            gender = None\n",
    "            if character_name in female_words:\n",
    "                gender = 'F'\n",
    "            elif character_name in male_words:\n",
    "                gender = 'M'\n",
    "            else:\n",
    "                for ix, char in char_list.iterrows():\n",
    "                    chk = str(char['char_name'])\n",
    "                    if character_name in chk.lower():\n",
    "                        gender = char['gender']\n",
    "                        break\n",
    "                    \n",
    "                if gender is None:\n",
    "                    gender = classify_gender(character_name)\n",
    "\n",
    "            governor = int(name['governor'])\n",
    "            governor_df = movie[(movie['token_id'] == governor) & (movie['dep_pos']=='JJ')]\n",
    "            df2 = movie[(movie['sentence_id']==name['sentence_id']) & (movie['governor'] == name['token_id'])  & (movie['dep_pos']=='JJ')]\n",
    "            df3 = movie[(movie['sentence_id'] == name['sentence_id']) & (movie['governor']==name['governor']) & (movie['dep_pos']=='JJ')]\n",
    "            y = pd.concat([governor_df, df2, df3])\n",
    "            for i, x in y.iterrows():\n",
    "                frequency_list[gender][x['dependent']] +=1\n",
    "        except Exception as exc:\n",
    "            pass\n",
    "        \n",
    "    return frequency_list\n",
    "    \n",
    "def get_name_and_adjective_mapping(all_movie_plots):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    pool = mp.Pool(20)\n",
    "    results = [pool.apply_async(get_frequency_for_movie, args=(movie,)) for movie in all_movie_plots]    \n",
    "    output = [p.get() for p in results]\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_adjective_cloud(filename):\n",
    "    movie_data_df = read_input_file(filename)\n",
    "    all_movie_plots = get_plots_by_movie_id(movie_data_df)\n",
    "    name_adj_cluster_list = get_name_and_adjective_mapping(all_movie_plots)\n",
    "    return name_adj_cluster_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = get_adjective_cloud('india.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_list = {'M':defaultdict(int),'F':defaultdict(int)}\n",
    "\n",
    "for freq in result:\n",
    "    for k, v in freq['M'].items():\n",
    "        frequency_list[k]+=v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Odds Ratio\n",
    "\n",
    "events = set()\n",
    "m_total = 0\n",
    "f_total = 0\n",
    "\n",
    "for key, value in frequency_list['M'].items():\n",
    "    events.add(key)\n",
    "    m_total += value\n",
    "    \n",
    "for key, value in frequency_list['F'].items():\n",
    "    events.add(key)\n",
    "    f_total += value\n",
    "    \n",
    "odds_ratio = {}\n",
    "for event in events:\n",
    "    odds_ratio[event] = (frequency_list['M'][event] * (f_total - frequency_list['F'][event]))/(frequency_list['F'][event] * (m_total - frequency_list['M'][event]))\n",
    "\n",
    "sorted_odd_ratio = {k: v for k, v in sorted(odds_ratio.items(), key=lambda item: item[1])}\n",
    "\n",
    "top_m = list(sorted_odd_ratio.keys())[:50]\n",
    "top_f = list(sorted_odd_ratio.keys())[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weat Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6_liysF8en"
   },
   "outputs": [],
   "source": [
    "def swAB(W, A, B):\n",
    "    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "    \"\"\"\n",
    "    WA = cosine_similarity(W,A)\n",
    "    WB = cosine_similarity(W,B)\n",
    "\n",
    "    #Take mean along columns\n",
    "    WAmean = np.mean(WA, axis = 1)\n",
    "    WBmean = np.mean(WB, axis = 1)\n",
    "\n",
    "    return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "    \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "    \"\"\"\n",
    "    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHPKwHLjo-m8"
   },
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B, embd):\n",
    "    \"\"\"Computes the effect size for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "     Returns\n",
    "              Effect Size\n",
    "    \"\"\"\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    XuY = list(set(X).union(Y))\n",
    "    XuYmat = []\n",
    "    for w in XuY:\n",
    "    if w.lower() in embd:\n",
    "        XuYmat.append(embd[w.lower()])\n",
    "    XuYmat = np.array(XuYmat)\n",
    "\n",
    "\n",
    "    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDy-duFOFj71"
   },
   "outputs": [],
   "source": [
    "def random_permutation(iterable, r=None):\n",
    "    \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    r = len(pool) if r is None else r\n",
    "    return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
    "    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "    \"\"\"\n",
    "    size_of_permutation = min(len(X), len(Y))\n",
    "    X_Y = X + Y\n",
    "    test_stats_over_permutation = []\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    if not sample:\n",
    "        permutations = combinations(X_Y, size_of_permutation)\n",
    "    else:\n",
    "        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "\n",
    "    for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "\n",
    "    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upscdvtwFm0l"
   },
   "outputs": [],
   "source": [
    "\"\"\"List of association and target word pairs for the sample test (top_m, top_f) vs (Pleasant, Unpleasant)\"\"\"\n",
    "\n",
    "\n",
    "X = top_m\n",
    "\n",
    "Y = top_f\n",
    "\n",
    "# Find better pleasant and unpleasant words\n",
    "\n",
    "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
    "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
    "\n",
    "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
    "    \"vomit\", \"agony\", \"prison\"] \n",
    "\n",
    "\n",
    "resourceFile = ''\n",
    "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
    "print('The glove embedding has been loaded!')\n",
    "\n",
    "\"\"\"Compute the effect-size and P value\"\"\"\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
