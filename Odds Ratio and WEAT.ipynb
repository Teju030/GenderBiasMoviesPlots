{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get adjectives related to male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core import indexing\n",
    "from gender_predictor.GenderClassifier import classify_gender\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = pd.read_csv('character.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'release_date','char_name', 'dob', 'gender', 'height', 'ethnicity', 'name', 'age', 'free_char_id1', 'free_char_id2', 'free_char_id3'])\n",
    "character = character[['id', 'char_name', 'gender']]\n",
    "\n",
    "\n",
    "female_words = ['she', 'her', 'woman', 'women', 'ladies', 'girls', 'lady', 'female', 'girl', 'damsel', 'maiden', 'daughter', 'sister', 'mother']\n",
    "male_words = ['he', 'his', 'man', 'male', 'men', 'boys', 'gentleman', 'gentlemen', 'boy', 'bloke', 'brother', 'father']\n",
    "\n",
    "count = 0\n",
    "def read_input_file(filename):\n",
    "    data_df = pd.read_csv(filename,sep=',', skip_blank_lines=True, index_col= False)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_plots_by_movie_id(data_df):\n",
    "    movie_ids = data_df.movie_id.unique() \n",
    "    grouped = data_df.groupby(data_df.movie_id)\n",
    "\n",
    "    all_movie_plots = []\n",
    "    for id in movie_ids:\n",
    "        sents_df = grouped.get_group(id)\n",
    "        all_movie_plots.append(sents_df)\n",
    "    return all_movie_plots[0:1000]\n",
    "\n",
    "\n",
    "def get_frequency_for_movie(movie):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    name_data  = movie[((movie.dep_pos == 'NNP') & (movie.dep_ner == 'PERSON')) | (movie.dependent.isin(female_words)) | (movie.dependent.isin(male_words))]\n",
    "    char_list  = character[character.id==movie.iloc[0]['movie_id']]\n",
    "    for idx,name in name_data.iterrows():            \n",
    "        try:\n",
    "            character_name = name['dependent'].lower()\n",
    "            gender = None\n",
    "            if character_name in female_words:\n",
    "                gender = 'F'\n",
    "            elif character_name in male_words:\n",
    "                gender = 'M'\n",
    "            else:\n",
    "                for ix, char in char_list.iterrows():\n",
    "                    chk = str(char['char_name'])\n",
    "                    if character_name in chk.lower():\n",
    "                        gender = char['gender']\n",
    "                        break\n",
    "                    \n",
    "                if gender is None:\n",
    "                    gender = classify_gender(character_name)\n",
    "        \n",
    "            governor = int(name['governor'])\n",
    "            governor_df = movie[(movie['sentence_id']==name['sentence_id']) & (movie['token_id'] == governor) & (movie['dep_pos']=='JJ')]\n",
    "            df2 = movie[(movie['sentence_id']==name['sentence_id']) & (movie['governor'] == name['token_id'])  & (movie['dep_pos']=='JJ')]\n",
    "            df3 = movie[(movie['sentence_id'] == name['sentence_id']) & (movie['governor']==name['governor']) & (movie['dep_pos']=='JJ')]\n",
    "            y = pd.concat([governor_df, df2, df3]).drop_duplicates()\n",
    "            for i, x in y.iterrows():\n",
    "                frequency_list[gender][x['dependent']] +=1\n",
    "        except Exception as exc:\n",
    "            pass\n",
    "    print(movie.iloc[0]['movie_id'])\n",
    "    print(frequency_list)\n",
    "    return frequency_list\n",
    "    \n",
    "def get_name_and_adjective_mapping(all_movie_plots):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    pool = mp.Pool(10)\n",
    "    results = [pool.apply_async(get_frequency_for_movie, args=(movie,)) for movie in all_movie_plots]    \n",
    "    output = [p.get() for p in results]\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_adjective_cloud(filename):\n",
    "    movie_data_df = read_input_file(filename)\n",
    "    all_movie_plots = get_plots_by_movie_id(movie_data_df)\n",
    "    name_adj_cluster_list = get_name_and_adjective_mapping(all_movie_plots)\n",
    "    return name_adj_cluster_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.971110\n",
      "Accuracy: 0.970850\n",
      "Accuracy: 0.969653Accuracy: 0.969601\n",
      "\n",
      "Accuracy: 0.970538\n",
      "Accuracy: 0.968091Accuracy: 0.968456\n",
      "\n",
      "975900Accuracy: 0.970069\n",
      "{'M': defaultdict(<class 'int'>, {'many': 1, 'fierce': 1}), 'F': defaultdict(<class 'int'>, {})}\n",
      "\n",
      "Accuracy: 0.970434\n",
      "Accuracy: 0.968664\n",
      "4951456\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {'American': 1})}\n",
      "Accuracy: 0.969757\n",
      "25960460\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {'previous': 1})}\n",
      "Accuracy: 0.968195\n",
      "Accuracy: 0.967779\n",
      "Accuracy: 0.969965\n",
      "Accuracy: 0.970590\n",
      "Accuracy: 0.967779\n",
      "Accuracy: 0.968508\n",
      "156558\n",
      "{'M': defaultdict(<class 'int'>, {'unemployed': 1, 'subject': 1, 'unsuccessful': 2, 'mature': 1, 'own': 1, 'pregnant': 1, 'third': 1, 'old': 1}), 'F': defaultdict(<class 'int'>, {'other': 1, 'much': 1, 'pregnant': 1, 'second': 1})}\n",
      "Accuracy: 0.968247\n",
      "Accuracy: 0.969913\n",
      "Accuracy: 0.970954\n",
      "Accuracy: 0.970590\n",
      "Accuracy: 0.970121\n",
      "Accuracy: 0.970382\n",
      "Accuracy: 0.970173\n",
      "Accuracy: 0.968351\n",
      "Accuracy: 0.970017\n",
      "Accuracy: 0.968924\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.967779\n",
      "Accuracy: 0.972151\n",
      "Accuracy: 0.968351\n",
      "Accuracy: 0.967831\n",
      "12053509\n",
      "{'M': defaultdict(<class 'int'>, {'rebellious': 1, 'pitiful': 1, 'own': 2, 'stylish': 1, 'handsome': 1, 'Italian': 1, 'married': 1, 'Asian': 1, 'other': 2, 'gay': 2, 'unsuspecting': 1}), 'F': defaultdict(<class 'int'>, {'middle-aged': 2, 'wealthy': 2, 'Italian': 3, 'handsome': 1, 'married': 1, 'Asian': 1, 'other': 2, 'own': 1})}\n",
      "Accuracy: 0.968976\n",
      "Accuracy: 0.969809\n",
      "Accuracy: 0.968716\n",
      "Accuracy: 0.968976\n",
      "Accuracy: 0.967623\n",
      "Accuracy: 0.970069\n",
      "Accuracy: 0.969601\n",
      "Accuracy: 0.967571\n",
      "Accuracy: 0.970538\n",
      "Accuracy: 0.967935Accuracy: 0.965957\n",
      "\n",
      "Accuracy: 0.968404\n",
      "Accuracy: 0.969861\n",
      "Accuracy: 0.968716\n",
      "Accuracy: 0.970017\n",
      "5708633\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.969497\n",
      "Accuracy: 0.969445\n",
      "Accuracy: 0.969132\n",
      "Accuracy: 0.966998\n",
      "Accuracy: 0.969028\n",
      "2487170\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.969184\n",
      "Accuracy: 0.968612\n",
      "12008535\n",
      "{'M': defaultdict(<class 'int'>, {'young': 1, 'eccentric': 1, 'full': 1, 'local': 1}), 'F': defaultdict(<class 'int'>, {'Maya': 1})}\n",
      "Accuracy: 0.970277\n",
      "Accuracy: 0.968247\n",
      "Accuracy: 0.969601\n",
      "Accuracy: 0.970486\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.969080\n",
      "Accuracy: 0.968456\n",
      "12371532\n",
      "{'M': defaultdict(<class 'int'>, {'middle-aged': 1}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.970486\n",
      "Accuracy: 0.969913\n",
      "Accuracy: 0.971058\n",
      "Accuracy: 0.970225\n",
      "Accuracy: 0.969028\n",
      "Accuracy: 0.970017\n",
      "Accuracy: 0.970329\n",
      "32104837\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.970017\n",
      "Accuracy: 0.970121\n",
      "Accuracy: 0.969340\n",
      "Accuracy: 0.967987\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.968768\n",
      "Accuracy: 0.969965\n",
      "Accuracy: 0.968560\n",
      "Accuracy: 0.971214\n",
      "Accuracy: 0.970017\n",
      "Accuracy: 0.969757\n",
      "Accuracy: 0.969913\n",
      "Accuracy: 0.968664\n",
      "Accuracy: 0.970954\n",
      "7028314Accuracy: 0.968560\n",
      "\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}Accuracy: 0.970017\n",
      "\n",
      "Accuracy: 0.970069\n",
      "Accuracy: 0.971475\n",
      "Accuracy: 0.967415\n",
      "Accuracy: 0.969965\n",
      "Accuracy: 0.971423\n",
      "Accuracy: 0.970902\n",
      "Accuracy: 0.969913Accuracy: 0.969497\n",
      "\n",
      "Accuracy: 0.970538\n",
      "Accuracy: 0.971371\n",
      "Accuracy: 0.970121\n",
      "Accuracy: 0.968924\n",
      "Accuracy: 0.968351\n",
      "Accuracy: 0.972620\n",
      "Accuracy: 0.969497\n",
      "Accuracy: 0.971162\n",
      "Accuracy: 0.970121\n",
      "Accuracy: 0.967467\n",
      "2647998\n",
      "{'M': defaultdict(<class 'int'>, {'new': 1, 'young': 1, 'local': 1}), 'F': defaultdict(<class 'int'>, {'local': 1, 'joint': 2})}\n",
      "Accuracy: 0.970746\n",
      "Accuracy: 0.969549\n",
      "Accuracy: 0.968039\n",
      "Accuracy: 0.971735\n",
      "Accuracy: 0.968195\n",
      "Accuracy: 0.969132\n",
      "Accuracy: 0.969497\n",
      "Accuracy: 0.971943\n",
      "Accuracy: 0.968143\n",
      "Accuracy: 0.970017\n",
      "Accuracy: 0.970590\n",
      "5664529\n",
      "{'M': defaultdict(<class 'int'>, {'own': 1}), 'F': defaultdict(<class 'int'>, {'own': 1})}\n",
      "Accuracy: 0.968976\n",
      "21926710\n",
      "{'M': defaultdict(<class 'int'>, {'10-year-old': 1, 'beautiful': 1}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.966842\n",
      "Accuracy: 0.969132\n",
      "Accuracy: 0.970850\n",
      "77856\n",
      "{'M': defaultdict(<class 'int'>, {'good': 1, 'cold': 1, 'loving': 1, 'firm': 1, 'jovial': 3, 'attempted': 1, 'own': 1, 'chimney-sweep': 1, 'somber': 1, 'old': 1, 'suicidal': 1, 'happy': 1}), 'F': defaultdict(<class 'int'>, {'Lead': 2, 'cold': 1, 'loving': 2, 'good': 1, 'strong': 1, 'magical': 1, 'stunned': 1, 'firm': 1, 'long': 1, 'childish': 1, 'irate': 1, 'late': 1, 'somber': 1, 'all-purpose': 1, 'relieved': 2, 'happy': 1})}\n",
      "Accuracy: 0.968924\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.969757\n",
      "Accuracy: 0.968299\n",
      "Accuracy: 0.969393\n",
      "Accuracy: 0.969288\n",
      "Accuracy: 0.970173\n",
      "12788657\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.970382\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.969861\n",
      "Accuracy: 0.968560\n"
     ]
    }
   ],
   "source": [
    "%time result = get_adjective_cloud('usa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save frequency\n",
    "import pickle\n",
    "output = open('usa_freq_1.pkl', 'wb')\n",
    "pickle.dump(frequency_list, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('india_freq_1.pkl', 'rb')\n",
    "\n",
    "frequency_list = pickle.load(pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine frequency from different results\n",
    "frequency_list = {'M':defaultdict(int),'F':defaultdict(int)}\n",
    "\n",
    "for freq in result:\n",
    "    for k, v in freq['M'].items():\n",
    "        frequency_list['M'][k]+=v\n",
    "    for k, v in freq['F'].items():\n",
    "        frequency_list['F'][k]+=v  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Odds Ratio\n",
    "\n",
    "events = set()\n",
    "m_total = 0\n",
    "f_total = 0\n",
    "\n",
    "for key, value in frequency_list['M'].items():\n",
    "    events.add(key)\n",
    "    m_total += value\n",
    "    \n",
    "for key, value in frequency_list['F'].items():\n",
    "    events.add(key)\n",
    "    f_total += value\n",
    "    \n",
    "odds_ratio = {}\n",
    "for event in events:\n",
    "    try:\n",
    "        odds_ratio[event] = (frequency_list['M'][event] * (f_total - frequency_list['F'][event]))/(frequency_list['F'][event] * (m_total - frequency_list['M'][event]))\n",
    "    except:\n",
    "        odds_ratio[event] = 0\n",
    "        pass\n",
    "    \n",
    "sorted_odd_ratio = {k: v for k, v in sorted(odds_ratio.items(), key=lambda item: item[1])}\n",
    "\n",
    "top_m = list(sorted_odd_ratio.keys())[:100]\n",
    "top_f = list(sorted_odd_ratio.keys())[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pitched',\n",
       " 'fatherlike',\n",
       " 'Vikrant',\n",
       " 'devil-worshipping',\n",
       " 'merry',\n",
       " 'Mythili',\n",
       " 'fabulous',\n",
       " 'scenic',\n",
       " 'astounded',\n",
       " 'senile',\n",
       " 'brain-dead',\n",
       " 'captive',\n",
       " 'unorthodox',\n",
       " 'fishy',\n",
       " 'immediate',\n",
       " 'territorial',\n",
       " 'truthful',\n",
       " 'generous',\n",
       " 'dejected',\n",
       " 'homeless',\n",
       " 'Ankush',\n",
       " 'quick',\n",
       " 'ardent',\n",
       " 'decent',\n",
       " 'deceased',\n",
       " 'unofficial',\n",
       " 'powerful',\n",
       " 'false',\n",
       " 'Nirmal',\n",
       " 'unsuspecting',\n",
       " 'worth',\n",
       " 'loud',\n",
       " 'slow',\n",
       " 'pitch-dark',\n",
       " 'grim',\n",
       " 'oppressed',\n",
       " 'graduate',\n",
       " 'precious',\n",
       " 'Aliyah',\n",
       " 'impressive',\n",
       " 'helter-skelter',\n",
       " 'wide-eyed',\n",
       " 'undying',\n",
       " 'spellbound',\n",
       " 'non-linear',\n",
       " 'Hilarious',\n",
       " 'heart-to-heart',\n",
       " 'hallucinating',\n",
       " 'leftist',\n",
       " 'frantic',\n",
       " 'Muni',\n",
       " 'front',\n",
       " 'depressed',\n",
       " 'vast',\n",
       " 'Devanathan',\n",
       " 'anti-national',\n",
       " 'Good',\n",
       " 'dignified',\n",
       " 'Eashwar',\n",
       " 'imaginary',\n",
       " 'voice-over',\n",
       " 'secluded',\n",
       " 'dissolute',\n",
       " 'extra',\n",
       " 'subsequent',\n",
       " 'scared',\n",
       " 'grown-up',\n",
       " 'unnamed',\n",
       " 'vicious',\n",
       " 'startling',\n",
       " 'Shankaran',\n",
       " 'left',\n",
       " 'total',\n",
       " 'Devdas',\n",
       " 'good-hearted',\n",
       " 'unavailable',\n",
       " 'Harshwardhan',\n",
       " 'stern',\n",
       " 'bent',\n",
       " 'rocket-propelled',\n",
       " 'subnormal',\n",
       " 'submachine',\n",
       " 'bluffing',\n",
       " 'constable',\n",
       " 'mild',\n",
       " 'impatient',\n",
       " 'civilian',\n",
       " 'brown',\n",
       " 'fauladi',\n",
       " 'humiliating',\n",
       " 'rural',\n",
       " 'social',\n",
       " 'faithful',\n",
       " 'lower-middle-class',\n",
       " 'Kalabhairavan',\n",
       " 'sadistic',\n",
       " 'Perumal',\n",
       " 'Malaysian',\n",
       " 'anonymous',\n",
       " 'spiritual']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weat Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6_liysF8en"
   },
   "outputs": [],
   "source": [
    "def swAB(W, A, B):\n",
    "    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "    \"\"\"\n",
    "    WA = cosine_similarity(W,A)\n",
    "    WB = cosine_similarity(W,B)\n",
    "\n",
    "    #Take mean along columns\n",
    "    WAmean = np.mean(WA, axis = 1)\n",
    "    WBmean = np.mean(WB, axis = 1)\n",
    "\n",
    "    return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "    \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "    \"\"\"\n",
    "    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHPKwHLjo-m8"
   },
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B, embd):\n",
    "    \"\"\"Computes the effect size for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "     Returns\n",
    "              Effect Size\n",
    "    \"\"\"\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    XuY = list(set(X).union(Y))\n",
    "    XuYmat = []\n",
    "    for w in XuY:\n",
    "    if w.lower() in embd:\n",
    "        XuYmat.append(embd[w.lower()])\n",
    "    XuYmat = np.array(XuYmat)\n",
    "\n",
    "\n",
    "    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDy-duFOFj71"
   },
   "outputs": [],
   "source": [
    "def random_permutation(iterable, r=None):\n",
    "    \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    r = len(pool) if r is None else r\n",
    "    return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
    "    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "    \"\"\"\n",
    "    size_of_permutation = min(len(X), len(Y))\n",
    "    X_Y = X + Y\n",
    "    test_stats_over_permutation = []\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    if not sample:\n",
    "        permutations = combinations(X_Y, size_of_permutation)\n",
    "    else:\n",
    "        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "\n",
    "    for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "\n",
    "    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upscdvtwFm0l"
   },
   "outputs": [],
   "source": [
    "\"\"\"List of association and target word pairs for the sample test (top_m, top_f) vs (Pleasant, Unpleasant)\"\"\"\n",
    "\n",
    "\n",
    "X = top_m\n",
    "\n",
    "Y = top_f\n",
    "\n",
    "# Find better pleasant and unpleasant words\n",
    "\n",
    "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
    "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
    "\n",
    "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
    "    \"vomit\", \"agony\", \"prison\"] \n",
    "\n",
    "\n",
    "resourceFile = ''\n",
    "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
    "print('The glove embedding has been loaded!')\n",
    "\n",
    "\"\"\"Compute the effect-size and P value\"\"\"\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
