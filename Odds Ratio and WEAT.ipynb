{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get adjectives related to male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core import indexing\n",
    "from gender_predictor.GenderClassifier import classify_gender\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv('movie.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'movie_name', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = pd.read_csv('character.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'release_date','char_name', 'dob', 'gender', 'height', 'ethnicity', 'name', 'age', 'free_char_id1', 'free_char_id2', 'free_char_id3'])\n",
    "character = character[['id', 'char_name', 'gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data['release_year'] = movie_data['release_date'].apply(lambda r:r[:4] if str(r)!='nan' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_by_year = {'United States of America':{}, 'India':{}}\n",
    "\n",
    "for index, row in movie_data.iterrows():\n",
    "    for key, value in json.loads(row['countries']).items():            \n",
    "        if value == 'United States of America' or value == 'India':\n",
    "            if row['release_year'] not in movie_id_by_year[value]:\n",
    "                movie_id_by_year[value][row['release_year']] = [row.id]\n",
    "            else:\n",
    "                movie_id_by_year[value][row['release_year']].append(row.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [i for i in movie_id_by_year['India'].keys() if i is not None and i <'2000' and i>='1990']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_1990 = []\n",
    "for c in count:\n",
    "    decade_1990 += movie_id_by_year['India'][c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decade_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_words = ['she', 'her', 'woman', 'women', 'ladies', 'girls', 'lady', 'female', 'girl', 'damsel', 'maiden', 'daughter', 'sister', 'mother']\n",
    "male_words   = ['he', 'his', 'man', 'male', 'men', 'boys', 'gentleman', 'gentlemen', 'boy', 'bloke', 'brother', 'father']\n",
    "\n",
    "\n",
    "def read_input_file(filename):\n",
    "    data_df = pd.read_csv(filename,sep=',', skip_blank_lines=True, index_col= False)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_plots_by_movie_id(data_df):\n",
    "    movie_ids = data_df.movie_id.unique() \n",
    "    grouped = data_df.groupby(data_df.movie_id)\n",
    "\n",
    "    all_movie_plots = []\n",
    "    for id in movie_ids:\n",
    "        sents_df = grouped.get_group(id)\n",
    "        all_movie_plots.append(sents_df)\n",
    "    return all_movie_plots\n",
    "\n",
    "\n",
    "def get_frequency_for_movie(movie):\n",
    "    \n",
    "    if movie.iloc[0]['movie_id'] in decade_1990:\n",
    "        return\n",
    "    \n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    name_data  = movie[((movie.dep_pos == 'NNP') & (movie.dep_ner == 'PERSON')) | (movie.dependent.isin(female_words)) | (movie.dependent.isin(male_words))]\n",
    "    char_list  = character[character.id==movie.iloc[0]['movie_id']]\n",
    "    \n",
    "    gender_list = {}\n",
    "    for idx,name in name_data.iterrows():            \n",
    "        try:\n",
    "            character_name = name['dependent'].lower()\n",
    "            gender = None\n",
    "            if character_name in gender_list:\n",
    "                gender = gender_list[character_name]\n",
    "            elif character_name in female_words:\n",
    "                gender = 'F'\n",
    "            elif character_name in male_words:\n",
    "                gender = 'M'\n",
    "            else:\n",
    "                for ix, char in char_list.iterrows():\n",
    "                    chk = str(char['char_name'])\n",
    "                    if character_name in chk.lower():\n",
    "                        gender = char['gender']\n",
    "                        break\n",
    "                    \n",
    "                if gender is None:\n",
    "                    gender = classify_gender(character_name)\n",
    "                    \n",
    "            gender_list[character_name] = gender\n",
    "            governor = int(name['governor'])\n",
    "            governor_df = movie[(movie['sentence_id']==name['sentence_id']) & (movie['token_id'] == governor) & (movie['dep_pos'].isin(['JJ', 'VB', 'VBP', 'VBZ', 'VBN']))]\n",
    "            df2 = movie[(movie['sentence_id']==name['sentence_id']) & (movie['governor'] == name['token_id'])  & (movie['dep_pos'].isin(['JJ', 'VB', 'VBP', 'VBZ', 'VBN']))]\n",
    "            df3 = movie[(movie['sentence_id'] == name['sentence_id']) & (movie['governor']==name['governor']) & (movie['dep_pos'].isin(['JJ', 'VB', 'VBP', 'VBZ', 'VBN']))]\n",
    "            y = pd.concat([governor_df, df2, df3]).drop_duplicates()\n",
    "            for i, x in y.iterrows():\n",
    "                frequency_list[gender][x['dependent']] +=1\n",
    "        except Exception as exc:\n",
    "            pass\n",
    "    print(movie.iloc[0]['movie_id'])\n",
    "    print(frequency_list)\n",
    "    return frequency_list\n",
    "    \n",
    "def get_name_and_adjective_mapping(all_movie_plots):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    pool = mp.Pool(20)\n",
    "    results = [pool.apply_async(get_frequency_for_movie, args=(movie,)) for movie in all_movie_plots]    \n",
    "    output = [p.get() for p in results]\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_adjective_cloud(filename):\n",
    "    movie_data_df = read_input_file(filename)\n",
    "    all_movie_plots = get_plots_by_movie_id(movie_data_df)\n",
    "    name_adj_cluster_list = get_name_and_adjective_mapping(all_movie_plots)\n",
    "    return name_adj_cluster_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20553214\n",
      "{'M': defaultdict(<class 'int'>, {'steal': 2}), 'F': defaultdict(<class 'int'>, {'offer': 1, 'aid': 1, 'make': 1})}\n",
      "Accuracy: 0.969757\n",
      "Accuracy: 0.970225\n",
      "26623942\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {'send': 1, 'be': 1, 'old': 1, 'pray': 1, 'appear': 1})}\n",
      "Accuracy: 0.969653Accuracy: 0.968247\n",
      "\n",
      "Accuracy: 0.967779Accuracy: 0.968091\n",
      "\n",
      "Accuracy: 0.969549Accuracy: 0.969132\n",
      "Accuracy: 0.968664\n",
      "\n",
      "Accuracy: 0.970694Accuracy: 0.968195\n",
      "\n",
      "Accuracy: 0.967779\n",
      "Accuracy: 0.970225\n",
      "32345990Accuracy: 0.967935\n",
      "Accuracy: 0.969757Accuracy: 0.969236{'M': defaultdict(<class 'int'>, {'see': 2, 'decide': 1, 'do': 1, 'end': 1, 'approve': 1, 'know': 1}), 'F': defaultdict(<class 'int'>, {'turn': 1, 'rich': 1, 'spoil': 1, 'bratty': 1, 'young': 1, 'conceit': 1, 'attract': 1, 'complicate': 1, 'be': 1, 'see': 1, 'sing': 2, 'true': 1})}Accuracy: 0.968091\n",
      "Accuracy: 0.968404\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.970798\n",
      "Accuracy: 0.966582\n",
      "23454846\n",
      "{'M': defaultdict(<class 'int'>, {'be': 10, 'have': 2, 'follow': 1, 'find': 4, 'sadistic': 1, 'drunk': 1, 'live': 1, 'swear': 1, 'protect': 1, 'arrest': 2, 'beat': 3, 'lock': 3, 'arrange': 2, 'get': 3, 'tell': 1, 'interested': 2, 'leave': 1, 'go': 1}), 'F': defaultdict(<class 'int'>, {'tell': 2, 'follow': 1, 'use': 1, 'aware': 1, 'be': 4, 'live': 2, 'have': 2, 'interested': 3, 'seek': 1})}\n",
      "Accuracy: 0.970590Accuracy: 0.968508\n",
      "\n",
      "Accuracy: 0.970902\n",
      "Accuracy: 0.970538\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.970798\n",
      "Accuracy: 0.969549Accuracy: 0.969757\n",
      "\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.968143\n",
      "Accuracy: 0.968351Accuracy: 0.968872\n",
      "\n",
      "Accuracy: 0.970694Accuracy: 0.969236\n",
      "\n",
      "21618909\n",
      "{'M': defaultdict(<class 'int'>, {'revolve': 1, 'be': 2, 'stoic': 1, 'meek': 1, 'exhibit': 1, 'think': 1, 'try': 1, 'sell': 1, 'get': 1, 'join': 1}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.969601\n",
      "Accuracy: 0.968143Accuracy: 0.970746\n",
      "\n",
      "Accuracy: 0.970382\n",
      "Accuracy: 0.970121\n",
      "4058835\n",
      "{'M': defaultdict(<class 'int'>, {'beg': 1, 'toss': 1, 'reject': 1, 'take': 1, 'end': 1, 'like': 1, 'do': 1, 'run': 3, 'be': 2, 'come': 2, 'have': 1, 'onboard': 1, 'see': 1, 'fall': 1, 'unite': 1}), 'F': defaultdict(<class 'int'>, {'leave': 2, 'be': 4, 'die': 2, 'wicked': 1, 'force': 1, 'beg': 1, 'teach': 1, 'lead': 1, 'find': 1, 'arrest': 2, 'take': 1, 'hungry': 2, 'slap': 1, 'drop': 2, 'rush': 1, 'adopt': 1, 'mope': 2, 'new': 1, 'run': 1, 'come': 1, 'have': 1, 'say': 1, 'see': 1})}\n",
      "11448183\n",
      "{'M': defaultdict(<class 'int'>, {'marry': 4, 'take': 2, 'have': 3, 'look': 1, 'hate': 1, 'fail': 2, 'convince': 1, 'get': 3, 'bring': 1, 'inform': 2, 'married': 1, 'ask': 1, 'arrange': 1, 'reject': 2, 'be': 4, 'hire': 1, 'know': 2, 'do': 1, 'claim': 2}), 'F': defaultdict(<class 'int'>, {'prefer': 1, 'name': 3, 'hate': 1, 'marry': 2, 'give': 2, 'thrill': 1, 'announce': 1, 'like': 1, 'approve': 1, 'ask': 1, 'do': 1, 'act': 1})}\n",
      "Accuracy: 0.968716\n",
      "20713032\n",
      "{'M': defaultdict(<class 'int'>, {}), 'F': defaultdict(<class 'int'>, {})}\n",
      "Accuracy: 0.969965\n",
      "Accuracy: 0.969653\n",
      "Accuracy: 0.968820\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.971006\n",
      "Accuracy: 0.969445\n",
      "Accuracy: 0.970798\n",
      "Accuracy: 0.971006\n",
      "Accuracy: 0.970902\n",
      "20904045Accuracy: 0.971006\n",
      "\n",
      "{'M': defaultdict(<class 'int'>, {'run': 1, 'meet': 1, 'know': 2, 'affix': 1, 'get': 1, 'prove': 1, 'innocent': 1, 'marry': 1}), 'F': defaultdict(<class 'int'>, {'have': 1, 'want': 1, 'run': 1, 'catch': 1, 'come': 1, 'live': 1, 'struggle': 1, 'leave': 2})}\n",
      "Accuracy: 0.968768\n",
      "Accuracy: 0.971162\n",
      "Accuracy: 0.969340\n",
      "Accuracy: 0.970382\n",
      "Accuracy: 0.968039\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.970434\n",
      "Accuracy: 0.970642Accuracy: 0.967883\n",
      "\n",
      "Accuracy: 0.970850\n",
      "10594195\n",
      "{'M': defaultdict(<class 'int'>, {'home-made': 1, 'be': 2, 'big': 1, 'bankrupt': 2, 'reveal': 2, 'go': 2, 'have': 2, 'turn': 1, 'encounter': 1, 'accompany': 1, 'reconstruct': 1}), 'F': defaultdict(<class 'int'>, {'sunny': 2, 'be': 3, 'young': 1, 'live': 1, 'have': 1, 'call': 1, 'spoil': 1, 'accustom': 1, 'surround': 1, 'accompany': 1, 'recognize': 1, 'do': 1, 'come': 1, 'know': 1, 'reconstruct': 1})}\n",
      "Accuracy: 0.969184\n",
      "Accuracy: 0.968247\n",
      "15698879\n",
      "{'M': defaultdict(<class 'int'>, {'see': 1, 'mysterious': 1, 'perfect': 1}), 'F': defaultdict(<class 'int'>, {'be': 1, 'beautiful': 1, 'true': 1, 'unclassy': 1, 'see': 1, 'have': 1, 'find': 1})}\n",
      "Accuracy: 0.967883Accuracy: 0.967883\n",
      "\n",
      "Accuracy: 0.970277\n",
      "Accuracy: 0.969184Accuracy: 0.971110\n",
      "\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.971735\n",
      "Accuracy: 0.969653\n",
      "Accuracy: 0.970329\n",
      "Accuracy: 0.968872\n",
      "29772142\n",
      "{'M': defaultdict(<class 'int'>, {'be': 3, 'famous': 1, 'own': 1, 'decide': 1, 'write': 1, 'have': 2, 'understand': 1, 'share': 1, 'tell': 1, 'lose': 1, 'final': 1, 'reach': 1, 'see': 1, 'bestow': 2}), 'F': defaultdict(<class 'int'>, {'understand': 1, 'tell': 1, 'want': 1, 'excite': 1, 'locate': 2, 'go': 1, 'native': 1, 'shock': 1, 'hear': 1, 'create': 1})}20604092\n",
      "\n",
      "{'M': defaultdict(<class 'int'>, {'meet': 3, 'help': 1, 'give': 1, 'successful': 1, 'find': 2, 'achieve': 1, 'decide': 1, 'reveal': 1, 'go': 1, 'hide': 1, 'set': 1, 'realize': 2, 'be': 3, 'have': 3, 'return': 2, 'tell': 2, 'confused': 1, 'come': 2, 'marry': 1}), 'F': defaultdict(<class 'int'>, {'meet': 4, 'help': 1, 'break': 1, 'decide': 1, 'many': 1, 'abandon': 1, 'go': 1, 'marry': 1, 'come': 1, 'future': 2, 'reveal': 1, 'realize': 2, 'be': 1})}\n",
      "Accuracy: 0.969705\n",
      "Accuracy: 0.969601\n",
      "Accuracy: 0.970121Accuracy: 0.969288\n",
      "Accuracy: 0.966634\n",
      "\n",
      "Accuracy: 0.969549\n",
      "Accuracy: 0.969393\n",
      "Accuracy: 0.969340\n",
      "20018854\n",
      "{'M': defaultdict(<class 'int'>, {'small': 1, 'be': 1, 'differ': 1}), 'F': defaultdict(<class 'int'>, {'pave': 1, 'acknowledge': 1, 'be': 7, 'start': 2, 'young': 1, 'dynamic': 1, 'self-motivated': 1, 'come': 1, 'know': 1, 'quiet': 1, 'force': 1, 'quit': 1, 'presume': 1, 'get': 4, 'boost': 1, 'try': 1, 'fool': 1, 'receive': 1, 'overall': 1, 'chalk': 1, 'ready': 1, 'disconcert': 1, 'feel': 1, 'promise': 2, 'have': 2, 'traumatize': 1, 'see': 1, 'leave': 1, 'bug': 1, 'visit': 1, 'apprehend': 1, 'attach': 1, 'decide': 1, 'face': 1})}\n",
      "7082486\n",
      "{'M': defaultdict(<class 'int'>, {'be': 4, 'make': 1, 'obtain': 1, 'go': 1, 'poor': 1, 'continue': 1, 'struggle': 1, 'have': 3, 'critical': 1, 'catch': 1, 'fall': 1, 'learn': 1, 'work': 1, 'secure': 1, 'accompany': 2, 'reveal': 1, 'turn': 1, 'arrange': 1, 'decide': 1, 'unable': 1, 'meet': 1, 'slap': 1, 'die': 1, 'manage': 1, 'help': 1, 'persuade': 1, 'come': 3, 'leave': 1, 'ask': 2, 'forget': 2, 'believe': 2, 'create': 1, 'overhear': 1, 'try': 1, 'force': 1, 'confess': 1, 'stop': 1, 'hesitate': 1, 'tell': 1, 'drive': 1, 'join': 1, 'remind': 2, 'late': 2, 'angry': 2, 'need': 1, 'happy': 1, 'live': 1}), 'F': defaultdict(<class 'int'>, {'name': 2, 'work': 1, 'turn': 3, 'be': 4, 'short-tempered': 1, 'young': 1, 'accompany': 1, 'arrange': 1, 'come': 1, 'meet': 1, 'slap': 1, 'reveal': 1, 'realize': 1, 'forget': 1, 'believe': 1, 'remind': 1})}\n",
      "Accuracy: 0.969809\n",
      "Accuracy: 0.968664\n",
      "Accuracy: 0.968716\n",
      "Accuracy: 0.970225\n",
      "Accuracy: 0.971423\n",
      "Accuracy: 0.969393\n",
      "Accuracy: 0.968820Accuracy: 0.970121\n",
      "\n",
      "Accuracy: 0.969497\n",
      "Accuracy: 0.970902Accuracy: 0.972203\n",
      "\n",
      "Accuracy: 0.969028\n",
      "Accuracy: 0.969236\n",
      "Accuracy: 0.968560\n",
      "Accuracy: 0.968820\n",
      "Accuracy: 0.968091\n",
      "Accuracy: 0.968299\n",
      "Accuracy: 0.967675\n",
      "Accuracy: 0.968976\n",
      "29528534\n",
      "Accuracy: 0.970538\n",
      "{'M': defaultdict(<class 'int'>, {'get': 1, 'old': 1, 'meet': 1, 'know': 3, 'be': 7, 'agree': 1, 'continue': 2, 'fall': 1, 'take': 2, 'come': 2, 'decide': 1, 'benevolent': 1, 'do': 1, 'irresponsible': 1, 'have': 1, 'marry': 1, 'discourage': 1, 'help': 1, 'grow': 2, 'own': 1, 'keep': 1}), 'F': defaultdict(<class 'int'>, {'live': 2, 'good': 1, 'be': 4, 'talent': 1, 'meet': 2, 'have': 2, 'struggle': 1, 'educate': 1, 'fall': 1, 'learn': 1, 'come': 1, 'sponsor': 1, 'know': 2, 'do': 1, 'write': 4, 'respond': 1, 'love': 1, 'ignore': 2, 'express': 1, 'discourage': 2, 'lead': 1, 'mad': 1, 's': 1, 'keep': 1, 'get': 1, 'dreamy': 1, 'share': 1, 'screen': 1, 'decide': 1, 'try': 1, 'stop': 2, 'firm': 2, 'remain': 1, 'prevent': 1})}\n"
     ]
    }
   ],
   "source": [
    "%time result = get_adjective_cloud('india_lemma.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save frequency\n",
    "import pickle\n",
    "output = open('india_2000_freq_1.pkl', 'wb')\n",
    "pickle.dump(frequency_list, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('india_2000_freq_1.pkl', 'rb')\n",
    "\n",
    "frequency_list = pickle.load(pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine frequency from different results\n",
    "frequency_list = {'M':defaultdict(int),'F':defaultdict(int)}\n",
    "\n",
    "for freq in result:\n",
    "    if freq is None:\n",
    "        continue\n",
    "    for k, v in freq['M'].items():\n",
    "        frequency_list['M'][k]+=v\n",
    "    for k, v in freq['F'].items():\n",
    "        frequency_list['F'][k]+=v  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio = {}\n",
    "threshold  = 2\n",
    "topk       = 50\n",
    "\n",
    "total_num_f = sum(frequency_list['F'].values())\n",
    "total_num_m = sum(frequency_list['M'].values())\n",
    "\n",
    "for key in frequency_list['F'].keys():\n",
    "    m_num = frequency_list['M'][key]\n",
    "    f_num = frequency_list['F'][key]\n",
    "    non_f_num = total_num_f - f_num\n",
    "    non_m_num = total_num_m - m_num\n",
    "    if f_num >= threshold and m_num >= threshold:\n",
    "        # we only consider the events where there are at least {thresohld} occurences for both gender\n",
    "        odds_ratio[key] = round((m_num / f_num) / (non_m_num / non_f_num), 2)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'senior': 5.44,\n",
       "  'old': 3.05,\n",
       "  'terrorist': 2.77,\n",
       "  'ready': 2.77,\n",
       "  'responsible': 2.56,\n",
       "  'drunk': 2.55,\n",
       "  'sure': 2.55,\n",
       "  'entire': 2.55,\n",
       "  'good': 2.47,\n",
       "  'due': 2.39,\n",
       "  'wrong': 2.34,\n",
       "  'successful': 2.34,\n",
       "  'last': 2.3,\n",
       "  'local': 2.23,\n",
       "  'unknown': 2.23,\n",
       "  'major': 2.02,\n",
       "  'present': 1.7,\n",
       "  'true': 1.6,\n",
       "  'small': 1.59,\n",
       "  'evil': 1.59,\n",
       "  'happy-go-lucky': 1.59,\n",
       "  'maternal': 1.59,\n",
       "  'rival': 1.59,\n",
       "  'high': 1.59,\n",
       "  'final': 1.59,\n",
       "  'suspicious': 1.49,\n",
       "  'free': 1.49,\n",
       "  'new': 1.45,\n",
       "  'real': 1.41,\n",
       "  'rich': 1.36,\n",
       "  'muslim': 1.35,\n",
       "  'much': 1.35,\n",
       "  'unable': 1.31,\n",
       "  'second': 1.27,\n",
       "  'helpless': 1.27,\n",
       "  'main': 1.27,\n",
       "  'arrogant': 1.27,\n",
       "  'anxious': 1.27,\n",
       "  'hard': 1.27,\n",
       "  'daily': 1.27,\n",
       "  'ashamed': 1.27,\n",
       "  'maya': 1.27,\n",
       "  'alcoholic': 1.27,\n",
       "  'past': 1.27,\n",
       "  'notorious': 1.27,\n",
       "  'clear': 1.27,\n",
       "  'afraid': 1.27,\n",
       "  'dev': 1.27,\n",
       "  'mock': 1.27,\n",
       "  'native': 1.27},\n",
       " {'beautiful': 0.09,\n",
       "  'female': 0.11,\n",
       "  'pregnant': 0.23,\n",
       "  'biological': 0.27,\n",
       "  'ill': 0.29,\n",
       "  'teenage': 0.32,\n",
       "  'shocked': 0.32,\n",
       "  'previous': 0.42,\n",
       "  'open': 0.42,\n",
       "  'secret': 0.42,\n",
       "  'social': 0.42,\n",
       "  'single': 0.42,\n",
       "  'close': 0.42,\n",
       "  'brilliant': 0.42,\n",
       "  'same': 0.46,\n",
       "  'twin': 0.48,\n",
       "  'mysterious': 0.48,\n",
       "  'shy': 0.48,\n",
       "  'jealous': 0.48,\n",
       "  'disappointed': 0.48,\n",
       "  'angry': 0.49,\n",
       "  'married': 0.52,\n",
       "  'poor': 0.54,\n",
       "  'beloved': 0.54,\n",
       "  'desperate': 0.54,\n",
       "  'different': 0.56,\n",
       "  'happy': 0.58,\n",
       "  'dead': 0.58,\n",
       "  'unaware': 0.59,\n",
       "  'relative': 0.64,\n",
       "  'outraged': 0.64,\n",
       "  'american': 0.64,\n",
       "  'middle-aged': 0.64,\n",
       "  'aged': 0.64,\n",
       "  'subordinate': 0.64,\n",
       "  'grand': 0.64,\n",
       "  'bad': 0.64,\n",
       "  'loving': 0.64,\n",
       "  'suitable': 0.64,\n",
       "  'lonely': 0.64,\n",
       "  'medical': 0.64,\n",
       "  'grateful': 0.64,\n",
       "  'elderly': 0.64,\n",
       "  'common': 0.64,\n",
       "  'unwilling': 0.64,\n",
       "  'well-known': 0.64,\n",
       "  'inseparable': 0.64,\n",
       "  'ambitious': 0.64,\n",
       "  'millionaire': 0.64,\n",
       "  'disturbed': 0.64})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "dict(sorted(odds_ratio.items(), key=itemgetter(1), reverse=True)[:topk]), dict(sorted(odds_ratio.items(), key=itemgetter(1))[:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fumbles',\n",
       " 'bowl',\n",
       " 'common',\n",
       " 'different',\n",
       " 'avoid',\n",
       " 'suffers',\n",
       " 'acquitted',\n",
       " 'melodious',\n",
       " 'breaks',\n",
       " 'born']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weat Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6_liysF8en"
   },
   "outputs": [],
   "source": [
    "def swAB(W, A, B):\n",
    "    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "    \"\"\"\n",
    "    WA = cosine_similarity(W,A)\n",
    "    WB = cosine_similarity(W,B)\n",
    "\n",
    "    #Take mean along columns\n",
    "    WAmean = np.mean(WA, axis = 1)\n",
    "    WBmean = np.mean(WB, axis = 1)\n",
    "\n",
    "    return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "    \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "    \"\"\"\n",
    "    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHPKwHLjo-m8"
   },
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B, embd):\n",
    "    \"\"\"Computes the effect size for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "     Returns\n",
    "              Effect Size\n",
    "    \"\"\"\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    XuY = list(set(X).union(Y))\n",
    "    XuYmat = []\n",
    "    for w in XuY:\n",
    "    if w.lower() in embd:\n",
    "        XuYmat.append(embd[w.lower()])\n",
    "    XuYmat = np.array(XuYmat)\n",
    "\n",
    "\n",
    "    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDy-duFOFj71"
   },
   "outputs": [],
   "source": [
    "def random_permutation(iterable, r=None):\n",
    "    \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    r = len(pool) if r is None else r\n",
    "    return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
    "    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "    \"\"\"\n",
    "    size_of_permutation = min(len(X), len(Y))\n",
    "    X_Y = X + Y\n",
    "    test_stats_over_permutation = []\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    if not sample:\n",
    "        permutations = combinations(X_Y, size_of_permutation)\n",
    "    else:\n",
    "        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "\n",
    "    for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "\n",
    "    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upscdvtwFm0l"
   },
   "outputs": [],
   "source": [
    "\"\"\"List of association and target word pairs for the sample test (top_m, top_f) vs (Pleasant, Unpleasant)\"\"\"\n",
    "\n",
    "\n",
    "X = top_m\n",
    "\n",
    "Y = top_f\n",
    "\n",
    "# Find better pleasant and unpleasant words\n",
    "\n",
    "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
    "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
    "\n",
    "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
    "    \"vomit\", \"agony\", \"prison\"] \n",
    "\n",
    "\n",
    "resourceFile = ''\n",
    "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
    "print('The glove embedding has been loaded!')\n",
    "\n",
    "\"\"\"Compute the effect-size and P value\"\"\"\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
