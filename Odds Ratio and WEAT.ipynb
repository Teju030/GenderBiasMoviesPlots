{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get adjectives related to male and female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bY4RMFQxrWyv",
    "outputId": "7926df4a-0561-4111-95a0-2a41ddc89e38"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations, filterfalse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.core import indexing\n",
    "from gender_predictor.GenderClassifier import classify_gender\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = pd.read_csv('character.metadata.tsv', sep='\\t', skip_blank_lines=True, header=None, names=['id', 'free_id', 'release_date','char_name', 'dob', 'gender', 'height', 'ethnicity', 'name', 'age', 'free_char_id1', 'free_char_id2', 'free_char_id3'])\n",
    "character = character[['id', 'char_name', 'gender']]\n",
    "\n",
    "\n",
    "female_words = ['she', 'her', 'woman', 'women', 'ladies', 'girls', 'lady', 'female', 'girl', 'damsel', 'maiden', 'daughter', 'sister', 'mother']\n",
    "male_words = ['he', 'his', 'man', 'male', 'men', 'boys', 'gentleman', 'gentlemen', 'boy', 'bloke', 'brother', 'father']\n",
    "\n",
    "count = 0\n",
    "def read_input_file(filename):\n",
    "    data_df = pd.read_csv(filename,sep=',', skip_blank_lines=True, index_col= False)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def get_plots_by_movie_id(data_df):\n",
    "    movie_ids = data_df.movie_id.unique() \n",
    "    grouped = data_df.groupby(data_df.movie_id)\n",
    "\n",
    "    all_movie_plots = []\n",
    "    for id in movie_ids:\n",
    "        sents_df = grouped.get_group(id)\n",
    "        all_movie_plots.append(sents_df)\n",
    "    return all_movie_plots[:1]\n",
    "\n",
    "\n",
    "def get_frequency_for_movie(movie):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    name_data  = movie[((movie.dep_pos == 'NNP') & (movie.dep_ner == 'PERSON')) | (movie.dependent.isin(female_words)) | (movie.dependent.isin(male_words))]\n",
    "    char_list  = character[character.id==movie.iloc[0]['movie_id']]\n",
    "    \n",
    "    gender_list = {}\n",
    "    for idx,name in name_data.iterrows():            \n",
    "        try:\n",
    "            character_name = name['dependent'].lower()\n",
    "            gender = None\n",
    "            if character_name in gender_list:\n",
    "                gender = gender_list[character_name]\n",
    "            elif character_name in female_words:\n",
    "                gender = 'F'\n",
    "            elif character_name in male_words:\n",
    "                gender = 'M'\n",
    "            else:\n",
    "                for ix, char in char_list.iterrows():\n",
    "                    chk = str(char['char_name'])\n",
    "                    if character_name in chk.lower():\n",
    "                        gender = char['gender']\n",
    "                        break\n",
    "                    \n",
    "                if gender is None:\n",
    "                    gender = classify_gender(character_name)\n",
    "                    \n",
    "            gender_list[character_name] = gender\n",
    "            governor = int(name['governor'])\n",
    "            governor_df = movie[(movie['sentence_id']==name['sentence_id']) & (movie['token_id'] == governor) & (movie['dep_pos']=='JJ')]\n",
    "            df2 = movie[(movie['sentence_id']==name['sentence_id']) & (movie['governor'] == name['token_id'])  & (movie['dep_pos']=='JJ')]\n",
    "            df3 = movie[(movie['sentence_id'] == name['sentence_id']) & (movie['governor']==name['governor']) & (movie['dep_pos']=='JJ')]\n",
    "            y = pd.concat([governor_df, df2, df3]).drop_duplicates()\n",
    "            for i, x in y.iterrows():\n",
    "                frequency_list[gender][x['dependent']] +=1\n",
    "        except Exception as exc:\n",
    "            pass\n",
    "    print(movie.iloc[0]['movie_id'])\n",
    "    print(frequency_list)\n",
    "    return frequency_list\n",
    "    \n",
    "def get_name_and_adjective_mapping(all_movie_plots):\n",
    "    frequency_list = {'M':defaultdict(int), 'F':defaultdict(int)}\n",
    "    pool = mp.Pool(1)\n",
    "    results = [pool.apply_async(get_frequency_for_movie, args=(movie,)) for movie in all_movie_plots]    \n",
    "    output = [p.get() for p in results]\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_adjective_cloud(filename):\n",
    "    movie_data_df = read_input_file(filename)\n",
    "    all_movie_plots = get_plots_by_movie_id(movie_data_df)\n",
    "    name_adj_cluster_list = get_name_and_adjective_mapping(all_movie_plots)\n",
    "    return name_adj_cluster_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968508\n",
      "prabhu\n",
      "M\n",
      "his\n",
      "M\n",
      "he\n",
      "M\n",
      "Accuracy: 0.969497\n",
      "pandya\n",
      "F\n",
      "prabhu\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "Accuracy: 0.967415\n",
      "yazhini\n",
      "F\n",
      "prabhu\n",
      "M\n",
      "his\n",
      "M\n",
      "man\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "yazhini\n",
      "F\n",
      "his\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "his\n",
      "M\n",
      "yazhini\n",
      "F\n",
      "she\n",
      "F\n",
      "Accuracy: 0.969080\n",
      "karuppu\n",
      "M\n",
      "her\n",
      "F\n",
      "she\n",
      "F\n",
      "her\n",
      "F\n",
      "yazhini\n",
      "F\n",
      "prabhu\n",
      "M\n",
      "karuppu\n",
      "M\n",
      "her\n",
      "F\n",
      "karuppu\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "his\n",
      "M\n",
      "karuppu\n",
      "M\n",
      "karuppu\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "he\n",
      "M\n",
      "man\n",
      "M\n",
      "he\n",
      "M\n",
      "he\n",
      "M\n",
      "his\n",
      "M\n",
      "his\n",
      "M\n",
      "prabhu\n",
      "M\n",
      "man\n",
      "M\n",
      "karuppu\n",
      "M\n",
      "her\n",
      "F\n",
      "he\n",
      "M\n",
      "her\n",
      "F\n",
      "prabhu\n",
      "M\n",
      "mother\n",
      "F\n",
      "her\n",
      "F\n",
      "daughter\n",
      "F\n",
      "pandya\n",
      "F\n",
      "karuppu\n",
      "M\n",
      "yazhini\n",
      "F\n",
      "karuppu\n",
      "M\n",
      "she\n",
      "F\n",
      "prabhu\n",
      "M\n",
      "20604092\n",
      "{'M': defaultdict(<class 'int'>, {'successful': 1, 'confused': 1}), 'F': defaultdict(<class 'int'>, {'many': 1, 'future': 2})}\n",
      "CPU times: user 1.05 s, sys: 133 ms, total: 1.18 s\n",
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%time result = get_adjective_cloud('india.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save frequency\n",
    "import pickle\n",
    "output = open('usa_freq_1.pkl', 'wb')\n",
    "pickle.dump(frequency_list, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('usa_freq_1.pkl', 'rb')\n",
    "\n",
    "frequency_list = pickle.load(pkl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine frequency from different results\n",
    "frequency_list = {'M':defaultdict(int),'F':defaultdict(int)}\n",
    "\n",
    "for freq in result:\n",
    "    for k, v in freq['M'].items():\n",
    "        frequency_list['M'][k]+=v\n",
    "    for k, v in freq['F'].items():\n",
    "        frequency_list['F'][k]+=v  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Odds Ratio\n",
    "\n",
    "events = set()\n",
    "m_total = 0\n",
    "f_total = 0\n",
    "\n",
    "for key, value in frequency_list['M'].items():\n",
    "    events.add(key)\n",
    "    m_total += value\n",
    "    \n",
    "for key, value in frequency_list['F'].items():\n",
    "    events.add(key)\n",
    "    f_total += value\n",
    "    \n",
    "odds_ratio = {}\n",
    "for event in events:\n",
    "    try:\n",
    "        odds_ratio[event] = (frequency_list['M'][event] * (f_total - frequency_list['F'][event]))/(frequency_list['F'][event] * (m_total - frequency_list['M'][event]))\n",
    "    except:\n",
    "        odds_ratio[event] = 0\n",
    "        pass\n",
    "    \n",
    "sorted_odd_ratio = {k: v for k, v in sorted(odds_ratio.items(), key=lambda item: item[1])}\n",
    "\n",
    "top_m = list(sorted_odd_ratio.keys())[:100]\n",
    "top_f = list(sorted_odd_ratio.keys())[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effeminate',\n",
       " 'necessary',\n",
       " 'bloody',\n",
       " 'fore-coming',\n",
       " 'impetuous',\n",
       " 'unfinished',\n",
       " 'heterosexual',\n",
       " 'knife-wielding',\n",
       " 'whirlwind',\n",
       " 'red-haired',\n",
       " 'self-defense',\n",
       " 'unfulfilled',\n",
       " 'fugitive',\n",
       " 'star-making',\n",
       " 'episodic',\n",
       " 'unfortunate',\n",
       " 'loveable',\n",
       " 'super-sized',\n",
       " 'infant',\n",
       " 'bewildered',\n",
       " 'bored',\n",
       " 'Speedy',\n",
       " 'intriguing',\n",
       " 'apparent',\n",
       " 'sassy',\n",
       " 'expensive-looking',\n",
       " 'career-minded',\n",
       " 'fifteen-year-old',\n",
       " 'attempted',\n",
       " 'worth',\n",
       " 'sane',\n",
       " 'well-traveled',\n",
       " 'disapproving',\n",
       " 'nervous',\n",
       " 'retired',\n",
       " 'smitten',\n",
       " 'Ole',\n",
       " 'rental',\n",
       " 'impeccable',\n",
       " 'shady',\n",
       " 'fading',\n",
       " 'pure',\n",
       " 'Teutonic',\n",
       " 'back',\n",
       " 'Beautiful',\n",
       " 'well-tailored',\n",
       " 'one-year-old',\n",
       " 'traceable',\n",
       " 'consecutive-game',\n",
       " 'immigrant',\n",
       " 'public',\n",
       " 'paralyzed',\n",
       " 'flirtatious',\n",
       " 'portable',\n",
       " 'confidential',\n",
       " 'hungover',\n",
       " 'already-distraught',\n",
       " 'elegant',\n",
       " 'callous',\n",
       " 'dance-hall',\n",
       " 'Distraught',\n",
       " 'unsatisfying',\n",
       " 'hotheaded',\n",
       " 'mobile',\n",
       " 'megalomaniacal',\n",
       " 'stressed',\n",
       " 'Aware',\n",
       " 'academic',\n",
       " 'past-his-prime',\n",
       " 'annoying',\n",
       " 'computer-controlled',\n",
       " 'blunt',\n",
       " 'Inisfree',\n",
       " 'tumbledown',\n",
       " 'mischief-causing',\n",
       " 'Oblivious',\n",
       " '3rd',\n",
       " 'Gedren',\n",
       " 'would-be',\n",
       " 'bereaved',\n",
       " 'bulletproof',\n",
       " 'mercurial',\n",
       " 'rotten',\n",
       " 'contemptuous',\n",
       " 'Imaginative',\n",
       " 'Legendary',\n",
       " 'heavy-hitting',\n",
       " 'genteel',\n",
       " 'Sure',\n",
       " 'numb',\n",
       " 'scandalous',\n",
       " 'flamboyant',\n",
       " 'folksy',\n",
       " 'Manhattan-based',\n",
       " 'high-spirited',\n",
       " 'depraved',\n",
       " 'dark-skinned',\n",
       " 'twice',\n",
       " 'okay',\n",
       " 'warm-hearted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weat Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf6_liysF8en"
   },
   "outputs": [],
   "source": [
    "def swAB(W, A, B):\n",
    "    \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
    "     Arguments\n",
    "              W, A, B : n x d matrix of word embeddings stored row wise\n",
    "    \"\"\"\n",
    "    WA = cosine_similarity(W,A)\n",
    "    WB = cosine_similarity(W,B)\n",
    "\n",
    "    #Take mean along columns\n",
    "    WAmean = np.mean(WA, axis = 1)\n",
    "    WBmean = np.mean(WB, axis = 1)\n",
    "\n",
    "    return (WAmean - WBmean)\n",
    "  \n",
    "def test_statistic(X, Y, A, B):\n",
    "    \"\"\"Calculates test-statistic between the pair of association words and target words\n",
    "     Arguments\n",
    "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
    "     Returns\n",
    "              Test Statistic\n",
    "    \"\"\"\n",
    "    return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHPKwHLjo-m8"
   },
   "outputs": [],
   "source": [
    "def weat_effect_size(X, Y, A, B, embd):\n",
    "    \"\"\"Computes the effect size for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "     Returns\n",
    "              Effect Size\n",
    "    \"\"\"\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    XuY = list(set(X).union(Y))\n",
    "    XuYmat = []\n",
    "    for w in XuY:\n",
    "    if w.lower() in embd:\n",
    "        XuYmat.append(embd[w.lower()])\n",
    "    XuYmat = np.array(XuYmat)\n",
    "\n",
    "\n",
    "    d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDy-duFOFj71"
   },
   "outputs": [],
   "source": [
    "def random_permutation(iterable, r=None):\n",
    "    \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
    "    pool = tuple(iterable)\n",
    "    r = len(pool) if r is None else r\n",
    "    return tuple(random.sample(pool, r))\n",
    "\n",
    "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
    "    \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
    "     Arguments\n",
    "              X, Y : List of association words\n",
    "              A, B : List of target words\n",
    "              embd : Dictonary of word-to-embedding for all words\n",
    "              sample : Number of random permutations used.\n",
    "     Returns\n",
    "    \"\"\"\n",
    "    size_of_permutation = min(len(X), len(Y))\n",
    "    X_Y = X + Y\n",
    "    test_stats_over_permutation = []\n",
    "\n",
    "    Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
    "    Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
    "    Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
    "    Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
    "\n",
    "    if not sample:\n",
    "        permutations = combinations(X_Y, size_of_permutation)\n",
    "    else:\n",
    "        permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
    "\n",
    "    for Xi in permutations:\n",
    "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
    "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
    "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
    "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
    "\n",
    "    unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
    "  \n",
    "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
    "  \n",
    "  return is_over.sum() / is_over.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upscdvtwFm0l"
   },
   "outputs": [],
   "source": [
    "\"\"\"List of association and target word pairs for the sample test (top_m, top_f) vs (Pleasant, Unpleasant)\"\"\"\n",
    "\n",
    "\n",
    "X = top_m\n",
    "\n",
    "Y = top_f\n",
    "\n",
    "# Find better pleasant and unpleasant words\n",
    "\n",
    "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
    "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
    "\n",
    "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
    "    \"vomit\", \"agony\", \"prison\"] \n",
    "\n",
    "\n",
    "resourceFile = ''\n",
    "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
    "print('The glove embedding has been loaded!')\n",
    "\n",
    "\"\"\"Compute the effect-size and P value\"\"\"\n",
    "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
    "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WEAT (Final).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
